{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "EXTRACTING DATA FROM A CSV FILE AND TXT FILE"
      ],
      "metadata": {
        "id": "FXDTMHezWd4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#Enter your dataset's name\n",
        "data = pd.read_csv(\"athlete.csv\")\n",
        "data"
      ],
      "metadata": {
        "id": "TY0ugHSKWkjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your text file's name\n",
        "with open('bike.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        print(line)"
      ],
      "metadata": {
        "id": "9_06Z_wFWnYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SINGLE WORD TOKENISATION USING FINDALL()"
      ],
      "metadata": {
        "id": "JICf9Iw9WsFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "# regular expression\n",
        "regex = r\"\\b[a-zA-Z]+\\b\"\n",
        "#Enter your dataset's name\n",
        "df = pd.read_csv('athlete.csv')\n",
        "tokens = []\n",
        "for text in df['Name']:\n",
        "    tokens.extend(re.findall(regex, text))\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "tGU5USVBWy3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NORMALISATION USING LOWER()"
      ],
      "metadata": {
        "id": "m44qOW8cZyeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#Enter your dataset's name\n",
        "df = pd.read_csv('athlete.csv')\n",
        "#Enter the Column you want to normalise\n",
        "df['Nationality'] = df['Nationality'].apply(lambda x: x.strip().lower())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "RTXH9CJAZ1vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVAL OF STOP WORDS AND STANDARDISATION"
      ],
      "metadata": {
        "id": "pfwTcv9namdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#Enter your text file's name\n",
        "with open('bike.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "words = text.split()\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "standardized_text = ' '.join(filtered_words)\n",
        "print(standardized_text)"
      ],
      "metadata": {
        "id": "68LfXHA9bfEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZING DATA USING TEXTBLOB"
      ],
      "metadata": {
        "id": "guVeqqgTdWIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "#Enter your dataset's name\n",
        "df = pd.read_csv('athlete.csv')\n",
        "def lemmatize_text(text):\n",
        "    blob = TextBlob(text)\n",
        "    lemmatized_words = []\n",
        "    for word in blob.words:\n",
        "        lemmatized_words.append(word.lemmatize())\n",
        "    return ' '.join(lemmatized_words)\n",
        "#enter the name of the column you want to lemmatize after df \n",
        "df['Name'] = df['Name'].apply(lemmatize_text)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "SIux861PdePc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMPUTING WORD FREQUENCY"
      ],
      "metadata": {
        "id": "gkdr-Hn0f3Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your filename\n",
        "filename = 'bike.txt'\n",
        "#Enter the word whose frequency you want to check\n",
        "word = 'Ducati'\n",
        "count = 0\n",
        "with open(filename, 'r') as file:\n",
        "    for line in file:\n",
        "        count += line.split().count(word)\n",
        "print(f'The word \"{word}\" appears {count} times in the file \"{filename}\"')"
      ],
      "metadata": {
        "id": "eVex_7gTf7i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEB SCRAPING"
      ],
      "metadata": {
        "id": "fRqCDlP3f8xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#Enter your URL here instead of the example one given below\n",
        "url = \"https://en.wikipedia.org/wiki/Ducati_Panigale_V4\"\n",
        "res = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(res.content, 'html.parser')\n",
        "links = soup.find_all('a')\n",
        "\n",
        "for link in links:\n",
        "    href = link.get('href')\n",
        "    if href is not None:\n",
        "      #Enter the word you want to search for instead of the goven word after 'if'\n",
        "        if 'Panigale' in href.lower():\n",
        "            print(href)"
      ],
      "metadata": {
        "id": "E8Lxz022f_iL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}